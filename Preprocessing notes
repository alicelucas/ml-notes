Reading csv file:
--> train_data = pd.read_csv(filename)

Removing missing values with pandas: 
--> train_data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)
--> if you'd like to look at a column (e.g. your target column), specify a subset argument.

Identify any missing columns:
--> cols_with_missing = [col for col in train_data.columns 
                                 if train_data[col].isnull().any()] 
--> You can now drop any column you'd like, for example:  train_data.drop(['Id'] + cols_with_missing, axis=1) 


Removing label column from data to obtain train data:
--> train_X = train_data.drop(['targetCol'], axis=1)

Only using continuous inputs or inputs with low levels of cardinality:
# "cardinality" means the number of unique values in a column.
# We use it as our only way to select categorical columns here. This is convenient, though
# a little arbitrary.
low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if 
                                candidate_train_predictors[cname].nunique() < 10 and
                                candidate_train_predictors[cname].dtype == "object"]
numeric_cols = [cname for cname in candidate_train_predictors.columns if 
                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]
my_cols = low_cardinality_cols + numeric_cols
train_X = train_X[my_cols]
test_X = test_X[my_cols]

Convert categorical to one-hot encoding:
one_hot_encoded_train_X = pd.get_dummies(train_X)
one_hot_encoded_test_X = pd.get_dummies(test_X)


Categoricals with many values: Scikit-learn's FeatureHasher uses the hashing trick to store high-dimensional data. 
This will add some complexity to your modeling code