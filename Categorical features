
Only using continuous inputs or inputs with low levels of cardinality:
# "cardinality" means the number of unique values in a column.
# We use it as our only way to select categorical columns here. This is convenient, though
# a little arbitrary.
low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if 
                                candidate_train_predictors[cname].nunique() < 10 and
                                candidate_train_predictors[cname].dtype == "object"]
numeric_cols = [cname for cname in candidate_train_predictors.columns if 
                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]
my_cols = low_cardinality_cols + numeric_cols
train_X = train_X[my_cols]
test_X = test_X[my_cols]

Convert categorical to one-hot encoding:
one_hot_encoded_train_X = pd.get_dummies(train_X)
one_hot_encoded_test_X = pd.get_dummies(test_X)


Categoricals with many values: Scikit-learn's FeatureHasher uses the hashing trick to store high-dimensional data. 
This will add some complexity to your modeling code


If using regression trees, converting categorical variables to continuous might not be necessary at all. 
Regression trees can take categorical values as inputs -- all that is needed is to define a splitting logic at a give node. 